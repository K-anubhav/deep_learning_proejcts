# Implementation of Backpropagation Algorithm and Gradient Checking from Scratch in Deep Learning.

# Objective:
The objective of this project was to implement Backpropagation from scratch and verify its correctness using a grader function. Additionally, the project required implementing two optimizers to update the weights and to understand the concept of gradient checking.

# Data:
There was no data involved in this project.

# Methodology:
The methodology of this project involved implementing Backpropagation and two optimizers - Stochastic Gradient Descent (SGD) and Momentum - from scratch. The implementation was then verified using a grader function that checked the correctness of the code.

# Step-wise approach:
The following steps were involved in this project:

1. Understanding the concept of Backpropagation and Gradient Checking.
2. Implementing Backpropagation from scratch.
3. Implementing two optimizers - SGD and Momentum.
4. Verifying the correctness of the implementation using a grader function.

# High-level statistics of the dataset:
There was no dataset involved in this project.

# Observations:
The implementation of Backpropagation and two optimizers was successful, and the grader function verified the correctness of the code. Additionally, the concept of gradient checking was understood, which helped in verifying the implementation.

# Conclusion:
In conclusion, this project helped in understanding the concept of Backpropagation and Gradient Checking and implementing them from scratch. Additionally, two optimizers were implemented to update the weights, and the implementation was verified using a grader function. The project was successful in achieving its objective.
